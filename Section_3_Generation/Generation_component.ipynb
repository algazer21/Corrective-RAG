{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System\n",
    "#### Alan García Zermeño\n",
    "06/13/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3:  Integrating the Generation Component\n",
    "#### This section includes:\n",
    "- Code snippets for integrating the generative model.\n",
    "- Examples of generated answers.\n",
    "- A brief discussion on the challenges faced and how they were addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import our script module for clean and load the database and our script for calling the retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Downloads/CRAG/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Import script modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../Scripts')))\n",
    "from datacleaner import data_cleaner\n",
    "from RetrievalSystem import evaluator, generGemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we use our data loader and cleaner. This function returns the complete corpus, and two arrays with the Questions and Answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Question/Answer pairs extracted!\n"
     ]
    }
   ],
   "source": [
    "corpus,questions,answers = data_cleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in this way, we will refer to our document retrieval system. The evaluator function receives the query to be evaluated, the corpus, and the responses from the corpus. It will return the most relevant response to the query if our system determines that it clearly answers the query; otherwise, it will return a boolean variable: *False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following document have exact information to answer the following query?\n",
      "    Please choose one of the two possible options: Yes, or No.\n",
      "    \n",
      "\n",
      "        Question: How effective is Keytruda in treating non-small cell lung cancer?\n",
      "\n",
      "        Document: Keytruda has shown to improve survival rates significantly in non-small cell lung cancer patients with PD-L1 expression.\n",
      "\n",
      "        Evaluation: [Select one: Yes, No]:\n",
      "Yes\n",
      "Keytruda has shown to improve survival rates significantly in non-small cell lung cancer patients with PD-L1 expression.\n"
     ]
    }
   ],
   "source": [
    "query = \"How effective is Keytruda in treating non-small cell lung cancer?\"\n",
    "context = evaluator(query,corpus,answers)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental idea of integrating the generative language model into the information retrieval system is to provide it with the context authorized by the evaluator model and ask it to provide its own response to the query within the context of the authorized response.\n",
    "Given that in the previous query, our evaluator model found an appropriate document and returned it in the 'context' variable, an example of a prompt to the generative model would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please generate an informative and concise response to the following query.\n",
    "Use the provided context information to ensure your response is accurate and relevant.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Response: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now call Gemini itself to generate a response based on this prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keytruda (pembrolizumab) has demonstrated significant effectiveness in treating non-small cell lung cancer (NSCLC) patients with PD-L1 expression. It has shown to improve survival rates by blocking the PD-1 protein on immune cells, allowing them to better recognize and attack cancer cells. Studies have found that Keytruda can extend overall survival and progression-free survival in patients with advanced NSCLC who have high PD-L1 expression.\n"
     ]
    }
   ],
   "source": [
    "if context:\n",
    "    print(generGemini(prompt))\n",
    "else:\n",
    "    print(\"I cant answer this prompt due a lack of proper information in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the ideal approach would be to use a different and more advanced model as the generative model. We will use the GPT API for using GPT-4o.\n",
    "\n",
    "- We will need an API key, so you can get one in: https://platform.openai.com/api-keys.\n",
    "- Then, save it in the 'gpt.txt' file in the APIS directory. \n",
    "- **Please only paste the API key**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "with open(\"../APIS/gpt.txt\", 'r') as file: apik = file.readline().strip()\n",
    "client = OpenAI(api_key = apik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GPT, we will use the same prompt but take advantage of the ability to break it down into different roles. We will also adjust the 'max_tokens = 120' parameter since this API is not free to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keytruda has demonstrated significant efficacy in treating non-small cell lung cancer patients with PD-L1 expression. Clinical studies have shown that Keytruda can improve survival rates and prolong progression-free survival in this patient population. It is considered a breakthrough treatment option for those with advanced non-small cell lung cancer, particularly when PD-L1 expression is present. However, the efficacy of Keytruda may vary based on individual patient factors and disease characteristics, so it is important to consult with a healthcare provider for personalized treatment recommendations.\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "prompt = f\"\"\"\n",
    "Please generate an informative and concise response to the following query.\n",
    "Use the provided context information to ensure your response is accurate and relevant.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "if context:\n",
    "  completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        max_tokens=120\n",
    "      )\n",
    "  print(completion.choices[0].message.content)\n",
    "else:\n",
    "    print(\"I cant answer this prompt due a lack of proper information in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a function to call the evaluator (Gemini) and the generator (GPT) and test some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRAG(query,corpus,answers,client):\n",
    "    \"\"\"\n",
    "    Calls the evaluator model given a query, a corpus and the set of answers.\n",
    "    Then, calls the generation model if the evaluator model autorizes.\n",
    "    Returns the system response.\n",
    "    Args:\n",
    "        query:      String query to evaluate\n",
    "        corpus:     Documents array to search for information\n",
    "        answers:    Answers array from the corpus without the questions\n",
    "        client:     GPT client\n",
    "    \"\"\"\n",
    "    #Calls the evaluator\n",
    "    context = evaluator(query,corpus,answers)\n",
    "\n",
    "    #Define model and prompt\n",
    "    model = \"gpt-3.5-turbo\"\n",
    "    prompt = f\"\"\"\n",
    "    Please generate an informative and concise response to the following query.\n",
    "    Use the provided context information to ensure your response is accurate and relevant.\n",
    "\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "    #If context != False, we will call GPT to generate a response given a query and the context\n",
    "    if context:\n",
    "        completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "                ],\n",
    "                max_tokens=120\n",
    "            )\n",
    "        print(\"\\n\\n GENERATOR RESPONSE:\")\n",
    "        print(completion.choices[0].message.content)\n",
    "    else:\n",
    "        print(\"I cant answer this prompt due a lack of proper information in the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following document have exact information to answer the following query?\n",
      "    Please choose one of the two possible options: Yes, or No.\n",
      "    \n",
      "\n",
      "        Question: what is Keytruda?\n",
      "\n",
      "        Document: Keytruda is administered as an intravenous infusion over 30 minutes.\n",
      "\n",
      "        Evaluation: [Select one: Yes, No]:\n",
      "No\n",
      "I cant answer this prompt due a lack of proper information in the database\n"
     ]
    }
   ],
   "source": [
    "query = \"what is Keytruda?\"\n",
    "CRAG(query,corpus,answers,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following document have exact information to answer the following query?\n",
      "    Please choose one of the two possible options: Yes, or No.\n",
      "    \n",
      "\n",
      "        Question: How long does it take to see the effects of Keytruda in treating cancer?\n",
      "\n",
      "        Document: Some patients may see effects as early as 2 to 3 months into the treatment.\n",
      "\n",
      "        Evaluation: [Select one: Yes, No]:\n",
      "Yes\n",
      "\n",
      "\n",
      " GENERATOR RESPONSE:\n",
      "Patients undergoing Keytruda treatment for cancer may start to see effects as early as 2 to 3 months into the treatment. This can vary among individuals and depend on several factors such as the type and stage of cancer being treated. It is important to regularly consult with your healthcare provider to monitor progress and discuss any changes or improvements in response to the treatment.\n"
     ]
    }
   ],
   "source": [
    "query = \"How long does it take to see the effects of Keytruda in treating cancer?\"\n",
    "CRAG(query,corpus,answers,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following document have exact information to answer the following query?\n",
      "    Please choose one of the two possible options: Yes, or No.\n",
      "    \n",
      "\n",
      "        Question: Can Keytruda be used in combination with other therapies?\n",
      "\n",
      "        Document: Yes, Keytruda can be used in combination with chemotherapy and other immunotherapies depending on the cancer type.\n",
      "\n",
      "        Evaluation: [Select one: Yes, No]:\n",
      "Yes\n",
      "\n",
      "\n",
      " GENERATOR RESPONSE:\n",
      "Yes, Keytruda can be used in combination with chemotherapy and other immunotherapies depending on the type of cancer being treated. It is often used in combination with other treatments to enhance its effectiveness in fighting cancer. It is important for healthcare providers to determine the most suitable combination therapy based on the specific cancer diagnosis and individual patient characteristics.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can Keytruda be used in combination with other therapies?\"\n",
    "CRAG(query,corpus,answers,client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Answers\n",
    "\n",
    "We can see that indeed when we input a query that the evaluator model authorizes, given that there is a clear answer in the corpus, the generative model (GPT) produces an appropriate and more detailed response to the query. The only thing missing in this system is implementing a web search when the evaluator model does not find an appropriate document in the database, so that we can also receive a response from the evaluator model in these cases. Again, we will implement this in section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the complications in implementing the generative model were:\n",
    "\n",
    "- There are answers in the database that do not really offer useful data, for example *40- Question: Can Keytruda cause fatigue in NSCLC patients?: Answer: No detailed information aviable on the given topic.* Fortunately, the evaluator model does not allow these to pass to the generative model.\n",
    "\n",
    "- A prompt explaining the task had to be added before the query to make it clear to the generative model that it needed to improve the response, because without that explanation, the model often simply generated the response provided in the prompt.\n",
    "\n",
    "- To avoid exhausting resources, we used GPT-3.5 turbo as the generative model here. In the next evaluation section, we will use GPT-4o for better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
