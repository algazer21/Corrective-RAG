{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Augmented Generation (RAG) System\n",
    "#### Alan García Zermeño\n",
    "06/13/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4:  Evaluation and Optimization\n",
    "#### This section includes:\n",
    "- Code snippets for the evaluation and optimization process.\n",
    "- Evaluation results (e.g., tables, charts).\n",
    "- A summary of optimizations and their impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import textwrap\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import script modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../Scripts')))\n",
    "from datacleaner import data_cleaner\n",
    "from RetrievalSystemWebEval import evaluator, generGemini, CRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the CRAG system, we will first load and clean the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Question/Answer pairs extracted!\n"
     ]
    }
   ],
   "source": [
    "corpus,questions,answers = data_cleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will call a version of our CRAG with web search (check section 6) that will return all the final responses along with the two boolean variables that we will explain later.\n",
    "The main idea is to use all the questions in our database to evaluate our system, and we will save all the answers in the \"_answersRAG_\" array.\n",
    "\n",
    "Important: We use a two-second pass between requests to Gemini so as not to exceed its quota limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301964cbe1194a0f8c859fc48b3d1a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "websbool = []\n",
    "qbool = []\n",
    "answersRAG  = []\n",
    "for que in tqdm(questions):\n",
    "    ans,errors,flag = CRAG(que,corpus,answers,safeURLs = ['keytruda.com','wikipedia'])\n",
    "    answersRAG.append(ans)\n",
    "    websbool.append(errors)\n",
    "    qbool.append(flag)\n",
    "    time.sleep(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"_websbool_\" array will save a boolean variable for each call to our CRAG, indicating whether information about pages marked as safe was found (in this case keytruda.com and wikipedia). The other variable \"_flag_\" is stored in the array \"_qbool_\" and tells us whether our evaluator model authorized a web search or not for each query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to evaluate our answers, we will compare them with the official answers in the database for each of the questions. The model you will evaluate will be a version of GPT. The prompt specifically contains the two answers, the question and asks the model to evaluate which of the two is better or if both are equally good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askGPT4(query,model):\n",
    "    \"\"\"\n",
    "    Evaluate between 2 responses.\n",
    "        Args:\n",
    "            query:  String with the 2 answers and the question\n",
    "            model:  String: version of the GPT model {gpt-4o,gpt-4} \n",
    "    \"\"\"  \n",
    "    with open(\"../APIS/gpt.txt\", 'r') as file: apik = file.readline().strip()\n",
    "    client = OpenAI(api_key = apik)\n",
    "\n",
    "    prompteval = \"\"\"Evaluate the following responses to the given question.\n",
    "    Your task is to determine which response is better or if both are kind of the same.\n",
    "    You can only respond JUST with ONE of the following options:\n",
    "\n",
    "    -  Response 1 is better\n",
    "    -  Response 2 is better\n",
    "    -  Both responses are kind of the same\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                {\"role\": \"system\", \"content\": prompteval},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "                ],\n",
    "                max_tokens=120\n",
    "            )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run our evaluation test by iteratively passing all the questions and their two answers, and we save the GOT evaluation in the \"_bestres_\" array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85388b07b7ee4339ba6a44edfa76803b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bestres = []\n",
    "for i in tqdm(range(len(questions))):\n",
    "    prompteval = f\"\"\"Question:\n",
    "    {questions[i]}\n",
    "\n",
    "    Response 1:\n",
    "    {answers[i]}\n",
    "\n",
    "    Response 2:\n",
    "    {answersRAG[i]}\n",
    "\n",
    "    Evaluation: \"\"\"\n",
    "    best = askGPT4(prompteval,model = \"gpt-4\")\n",
    "    bestres.append(best)\n",
    "    #print(f\"{i+1}.  {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define our 3 metrics to take into account based on all the responses:\n",
    "\n",
    "- **Retriever score (RS)**: The percentage of questions for which our retriever model managed to find a correct answer within the same database. Tells us how good our retriever system is.\n",
    "\n",
    "- **Web search score (WSS)**: On the occasions where the model had to access web search, it tells us the percentage of instances where we were able to obtain information from our secure pages (Wikipedia, Keytruda.com). It tells us how safe our web search is based on which web pages are marked as safe.\n",
    "\n",
    "- **CRAG score (CRAGS)**: Percentage of questions where CRAG answers were evaluated as \"better\" than those in the database by our chosen version of GPT. It tells us how much our CRAG improves on the original answers in the database.\n",
    "\n",
    "**First test**: First we will evaluate our CRAG where **GPT 3.5** is our generating model and we will have **two secure urls**: keytrude.com and wikipedia. The model that will qualify the answers will be **GPT-4o**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever score by evaluator model: 81.63%\n",
      "Web search score: 44.44%\n",
      "CRAG score by GPT4o: 91.84%\n"
     ]
    }
   ],
   "source": [
    "#gen by Gpt-3.5,secure webs and Gpt-4o eval\n",
    "two,one,same = 0,0,0\n",
    "for resp in bestres:\n",
    "    if '2' in resp:\n",
    "        two+=1\n",
    "    elif '1' in resp:\n",
    "        one+=1\n",
    "    else:\n",
    "        same+=1\n",
    "\n",
    "n = len(questions)\n",
    "print(f\"Retriever score by evaluator model: {round((n-sum(qbool))/n*100,2)}%\")\n",
    "print(f\"Secure webs score: {round((sum(qbool)-sum(websbool))*100/sum(qbool),2)}%\")\n",
    "print(f\"CRAG score by GPT4o: {round((two)*100/(n-same),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second test**: Next, we will evaluate our CRAG where **GPT-4o* is our generating model and we will **not have secure urls**, therefore, we will examine the internet without filters. The model that will qualify the answers will be **GPT-4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever score by evaluator model: 83.67%\n",
      "Secure webs score: 100.0%\n",
      "CRAG score by GPT4: 100.0%\n"
     ]
    }
   ],
   "source": [
    "#gen by Gpt-4o,no secure webs and Gpt-4 eval\n",
    "two,one,same = 0,0,0\n",
    "for resp in bestres:\n",
    "    if '2' in resp:\n",
    "        two+=1\n",
    "    elif '1' in resp:\n",
    "        one+=1\n",
    "    else:\n",
    "        same+=1\n",
    "\n",
    "n = len(questions)\n",
    "print(f\"Retriever score by evaluator model: {round((n-sum(qbool))/n*100,2)}%\")\n",
    "print(f\"Secure webs score: {round((sum(qbool)-sum(websbool))*100/sum(qbool),2)}%\")\n",
    "print(f\"CRAG score by GPT4: {round((two)*100/(n-same),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third test**: Next, we will evaluate our CRAG where **GPT-4* is our generating model and we will **not have secure urls**, therefore, we will examine the internet without filters. The model that will qualify the answers will be **GPT-4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever score by evaluator model: 81.63%\n",
      "Secure webs score: 100.0%\n",
      "CRAG score by GPT4: 75.86%\n"
     ]
    }
   ],
   "source": [
    "#gen by Gpt-4,no secure webs and Gpt-4 eval\n",
    "two,one,same = 0,0,0\n",
    "for resp in bestres:\n",
    "    if '2' in resp:\n",
    "        two+=1\n",
    "    elif '1' in resp:\n",
    "        one+=1\n",
    "    else:\n",
    "        same+=1\n",
    "\n",
    "n = len(questions)\n",
    "print(f\"Retriever score by evaluator model: {round((n-sum(qbool))/n*100,2)}%\")\n",
    "print(f\"Secure webs score: {round((sum(qbool)-sum(websbool))*100/sum(qbool),2)}%\")\n",
    "print(f\"CRAG score by GPT4: {round((two)*100/(n-same),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results\n",
    "\n",
    "**Test** | **RS** | **WSS** | **CRAGS** |\n",
    "|--------- |----------|----------|----------|\n",
    "| **Fist**    | 81.63%    | 44.44%   | 91.84%   |\n",
    "| **Second**    | 83.67%    | 100%   | 100%   |\n",
    "| **Third**    | 81.63%    | 100%   | 75.86   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations, Conclusions:\n",
    "- Since no corrections or improvements were made to the retriever system across the three tests, we can define that, on average, our retriever model achieved an **82.31%** success rate. This is a commendable result, which could be significantly enhanced by further enriching the database.\n",
    "\n",
    "- Our web search was consistently successful when we did not limit ourselves to secure websites only. We found results on secure sites 44.44% of the time when the filter was applied. This could be improved by expanding our list of secure websites.\n",
    "\n",
    "- Regarding the CRAG metric, we observe that when GPT-3.5 was our generating model, GPT-4 evaluated it with a score of 91%. When GPT-4 generated and was evaluated by itself, the score dropped to 75.86%. However, when GPT-4 generated and was evaluated by GPT-4o, we achieved the highest score. Therefore, we conclude that for this specific test, **GPT-4o is the most effective model to be used as the generating model for our CRAG**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
